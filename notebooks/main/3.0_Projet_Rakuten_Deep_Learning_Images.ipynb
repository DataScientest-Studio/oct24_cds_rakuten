{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CE NOTEBOOKE EST LA MISE AU PROPRE DU NOTEBOOK :\n",
        "#\n",
        "# 4.2-fh-modeling-advanced-image.ipynb\n",
        "#\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "DW6ycJYUpWFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**⚠️ Avertissement sur l'Exécution et l'Origine des Fichiers**\n",
        "\n",
        "L'entraînement de modèles de Deep Learning sur des images est une tâche très gourmande en ressources, qui requiert une puissance de calcul GPU importante. Pour cette raison, ce notebook n'a pas été ré-exécuté dans cet environnement.\n",
        "\n",
        "Par conséquent, les fichiers de **logits** (`gcvit_logits.pt`, `convnextv2_logits.pt`, etc.) et les modèles générés par ce script proviennent d'une exécution du notebook d'origine réalisée sur un serveur de calcul plus puissant et payant."
      ],
      "metadata": {
        "id": "i1dK8mb4ZOBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTIE 1 : Entraînement des Modèles de Computer Vision par lots"
      ],
      "metadata": {
        "id": "7aXTwkXvbBSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# NOTEBOOK D'ENTRAÎNEMENT PAR LOTS DE MODÈLES DE COMPUTER VISION\n",
        "#\n",
        "# Objectif : Préparer les données images, puis entraîner, évaluer et\n",
        "# sauvegarder une suite de modèles de classification d'images (Maxvit,\n",
        "# ConvNeXt, EfficientNet, etc.) de manière automatisée.\n",
        "#\n",
        "# Le code est structuré pour être modulaire et facilement extensible à de\n",
        "# nouvelles architectures de modèles.\n",
        "#\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "jaeNxFbdXE9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlms-x43XIBu",
        "outputId": "dde8eb21-9700-4686-f947-f4219c360c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montage de Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa3dogIuXJz6",
        "outputId": "8856531e-1aad-480b-a986-b3a405b36451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 1. PRÉPARATION DE L'ENVIRONNEMENT ======\n",
        "\n",
        "# Importation des bibliothèques\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import cv2\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import timm\n",
        "\n",
        "# PyTorch et Torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.autoaugment import TrivialAugmentWide\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "Qr-OI7iBXLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 2. PRÉPARATION DES DONNÉES IMAGES ======\n",
        "\n",
        "# Cette section décompresse les images depuis un fichier .zip sur Drive\n",
        "# vers l'espace local de Colab pour un accès plus rapide.\n",
        "\n",
        "# Définition des chemins\n",
        "zip_path = \"/content/drive/MyDrive/Colab Notebooks/images/image_train.zip\"\n",
        "temp_path = \"/content/temp_extraction/\"\n",
        "final_image_path = \"/content/images/\"\n",
        "\n",
        "# Création des dossiers nécessaires\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "os.makedirs(final_image_path, exist_ok=True)\n",
        "\n",
        "# Décompression du fichier zip dans le dossier temporaire\n",
        "print(\"Décompression des images...\")\n",
        "!unzip -q \"{zip_path}\" -d \"{temp_path}\"\n",
        "print(f\"✅ Zip extrait dans : {temp_path}\")\n",
        "\n",
        "def find_image_folder(base_dir):\n",
        "    \"\"\"\n",
        "    Parcourt un répertoire pour trouver le premier sous-dossier contenant des fichiers images.\n",
        "\n",
        "    Cette fonction est utile après une décompression où les images peuvent se trouver\n",
        "    dans un sous-dossier dont le nom n'est pas connu à l'avance (par exemple,\n",
        "    `/content/temp_extraction/image_train/`). Elle identifie le chemin\n",
        "    correct vers les images en se basant sur la présence de fichiers avec des\n",
        "    extensions courantes (.jpg, .png, etc.).\n",
        "\n",
        "    Args:\n",
        "        base_dir (str): Le chemin du répertoire de base où commencer la recherche.\n",
        "\n",
        "    Returns:\n",
        "        str | None: Le chemin complet du premier sous-dossier contenant des images,\n",
        "                    ou None si aucun dossier d'images n'est trouvé.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        if any(f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')) for f in files):\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "# Déplacement des images vers le dossier final\n",
        "image_root = find_image_folder(temp_path)\n",
        "if image_root:\n",
        "    moved_count = 0\n",
        "    for fname in os.listdir(image_root):\n",
        "        shutil.move(os.path.join(image_root, fname), os.path.join(final_image_path, fname))\n",
        "        moved_count += 1\n",
        "    print(f\"✅ {moved_count} images déplacées vers : {final_image_path}\")\n",
        "else:\n",
        "    print(\"❌ Aucune image trouvée après décompression.\")\n",
        "\n",
        "# Nettoyage du dossier temporaire\n",
        "shutil.rmtree(temp_path)\n",
        "print(\"🧹 Dossier temporaire supprimé.\")"
      ],
      "metadata": {
        "id": "vAjn7R8FXN1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 3. PRÉPARATION DU FICHIER CSV ======\n",
        "\n",
        "# Le CSV original contient des chemins qui ne sont pas valides dans Colab.\n",
        "# Cette section met à jour les chemins des images pour pointer vers le\n",
        "# dossier local `/content/images/` et sauvegarde ce nouveau CSV.\n",
        "\n",
        "print(\"\\nMise à jour des chemins dans le fichier CSV...\")\n",
        "df_original = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data_rakuten/X_train_fr_final.csv\")\n",
        "df_collab = df_original.copy()\n",
        "df_collab['image_path'] = df_collab['image_path'].apply(lambda p: os.path.join(final_image_path, os.path.basename(p)))\n",
        "collab_csv_path = '/content/X_train_fr_final_colab.csv'\n",
        "df_collab.to_csv(collab_csv_path, index=False)\n",
        "\n",
        "# Sauvegarde de la version modifiée sur Drive pour une utilisation future\n",
        "drive_csv_path = '/content/drive/MyDrive/Colab Notebooks/data_rakuten/X_train_fr_final_colab.csv'\n",
        "shutil.copy(collab_csv_path, drive_csv_path)\n",
        "print(f\"✅ CSV prêt et sauvegardé sur Drive : {drive_csv_path}\")"
      ],
      "metadata": {
        "id": "8Wj09j-mXQm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 4. CONFIGURATION CENTRALE ET CHARGEMENT DES DONNÉES ======\n",
        "\n",
        "# Configuration partagée pour tous les modèles\n",
        "CONFIG = {\n",
        "    \"max_epochs\": 30,\n",
        "    \"patience\": 7,\n",
        "    \"batch_size\": 32,\n",
        "    \"lr\": 1e-4,\n",
        "    \"weight_decay\": 1e-2,\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/data_rakuten\"\n",
        "DRIVE_OUTPUT_PATH = os.path.join(BASE_PATH, \"models_image\")\n",
        "os.makedirs(DRIVE_OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Chargement des données et des indices de séparation\n",
        "df = pd.read_csv(collab_csv_path)\n",
        "paths, labels = df[\"image_path\"].values, df[\"label\"].values\n",
        "with open(os.path.join(BASE_PATH, \"val_indices.json\"), \"r\") as f:\n",
        "    val_idx = np.array(json.load(f), dtype=int)\n",
        "train_idx = np.setdiff1d(np.arange(len(df)), val_idx)\n",
        "\n",
        "train_paths, train_labels = paths[train_idx], labels[train_idx]\n",
        "val_paths, val_labels = paths[val_idx], labels[val_idx]\n",
        "num_classes = len(np.unique(labels))\n",
        "\n",
        "print(f\"\\nConfiguration chargée. Utilisation de l'appareil : {DEVICE}\")\n",
        "print(f\"Nombre de classes : {num_classes}\")"
      ],
      "metadata": {
        "id": "NTrcgg8zXYnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 5. DÉFINITIONS COMMUNES (TRANSFORMS, DATASET) ======\n",
        "\n",
        "# Ces définitions sont partagées par tous les modèles pour assurer la cohérence.\n",
        "\n",
        "# Transformations d'images (avec augmentation pour l'entraînement)\n",
        "IMAGENET_MEAN, IMAGENET_STD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset PyTorch personnalisé pour charger des images et leurs labels à la volée.\n",
        "\n",
        "    Cette classe gère le chargement des images à partir de leurs chemins sur le disque,\n",
        "    la conversion de leur format de couleur (de BGR, utilisé par OpenCV, à RGB),\n",
        "    et l'application de transformations (par exemple, redimensionnement, augmentation\n",
        "    de données, normalisation) avant de les retourner pour l'entraînement ou l'évaluation.\n",
        "\n",
        "    Attributes:\n",
        "        paths (np.ndarray): Un tableau des chemins d'accès aux fichiers images.\n",
        "        labels (np.ndarray): Un tableau des labels numériques correspondants.\n",
        "        transform (callable): La pipeline de transformations à appliquer à chaque image.\n",
        "    \"\"\"\n",
        "    def __init__(self, paths, labels, transform):\n",
        "        \"\"\"\n",
        "        Initialise le Dataset.\n",
        "\n",
        "        Args:\n",
        "            paths (np.ndarray): La liste des chemins complets vers les images.\n",
        "            labels (np.ndarray): La liste des labels associés à chaque image.\n",
        "            transform (callable): Une fonction ou composition de transformations\n",
        "                                  (de torchvision.transforms) à appliquer à chaque image.\n",
        "        \"\"\"\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retourne le nombre total d'échantillons dans le dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: La taille totale du dataset.\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Récupère et retourne un échantillon (image transformée et son label) à un index donné.\n",
        "\n",
        "        Args:\n",
        "            i (int): L'index de l'échantillon à récupérer.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Un tuple contenant deux éléments :\n",
        "                   - torch.Tensor: L'image transformée sous forme de tenseur.\n",
        "                   - int: Le label numérique de l'image.\n",
        "        \"\"\"\n",
        "        img = cv2.imread(self.paths[i])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        return self.transform(img), int(self.labels[i])\n",
        "\n",
        "# Création des datasets\n",
        "train_dataset = ImageDataset(train_paths, train_labels, train_transforms)\n",
        "val_dataset = ImageDataset(val_paths, val_labels, val_transforms)\n",
        "full_dataset = ImageDataset(paths, labels, val_transforms)"
      ],
      "metadata": {
        "id": "fSrPO9s4XbEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 6. BOUCLE D'ENTRAÎNEMENT PRINCIPALE ======\n",
        "\n",
        "# Cette boucle itère sur une liste de noms de modèles `timm`.\n",
        "# Pour chaque modèle, elle exécute le cycle complet : entraînement, validation,\n",
        "# sauvegarde du meilleur état, et génération des logits.\n",
        "\n",
        "models_to_train = [\n",
        "    \"maxvit_base_tf_224.in21k\",\n",
        "    \"convnextv2_base.in22k\",\n",
        "    \"tf_efficientnetv2_l.in21k\",\n",
        "    \"coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k\",\n",
        "    \"gcvit_base.in1k\"\n",
        "]\n",
        "\n",
        "performance_history = {}\n",
        "\n",
        "for model_timm_name in models_to_train:\n",
        "    model_name = model_timm_name.split('.')[0]\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\" DÉBUT DE L'ENTRAÎNEMENT POUR : {model_name.upper()} \")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # --- Initialisation spécifique au modèle ---\n",
        "    model = timm.create_model(model_timm_name, pretrained=True, num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "    # --- Dataloader avec rééchantillonnage pour gérer le déséquilibre ---\n",
        "    counts = np.bincount(train_labels, minlength=num_classes)\n",
        "    sample_weights = 1.0 / counts[train_labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], sampler=sampler, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
        "\n",
        "    # --- Optimiseur, Scheduler et Fonction de Perte ---\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"max_epochs\"])\n",
        "    class_weights = torch.tensor(compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels), dtype=torch.float, device=DEVICE)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "    # --- Boucle d'entraînement et de validation ---\n",
        "    best_f1, wait = 0.0, 0\n",
        "    best_state = model.state_dict()\n",
        "    val_f1s = []\n",
        "\n",
        "    for ep in range(1, CONFIG[\"max_epochs\"] + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"{model_name} Ep{ep}\")\n",
        "        for x, y in pbar:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        scheduler.step()\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x = x.to(DEVICE)\n",
        "                logits = model(x)\n",
        "                y_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "                y_true.extend(y.numpy())\n",
        "        f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "        val_f1s.append(f1)\n",
        "        print(f\"  -> TrainLoss: {total_loss/len(train_loader):.4f} | Val F1_weighted: {f1:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            print(f\"   => Nouveau meilleur score F1 ! Sauvegarde du modèle.\")\n",
        "            best_f1, best_state, wait = f1, model.state_dict(), 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= CONFIG[\"patience\"]:\n",
        "                print(f\"⏹️ Arrêt anticipé à l'époque {ep}.\")\n",
        "                break\n",
        "\n",
        "    performance_history[model_name] = val_f1s\n",
        "\n",
        "    # --- Sauvegarde du meilleur modèle et des logits ---\n",
        "    print(f\"Sauvegarde du meilleur modèle {model_name} avec F1={best_f1:.4f}\")\n",
        "    model.load_state_dict(best_state)\n",
        "    torch.save(model.state_dict(), os.path.join(DRIVE_OUTPUT_PATH, f\"{model_name}_best.pt\"))\n",
        "\n",
        "    # --- Génération des logits sur le dataset complet ---\n",
        "    full_loader = DataLoader(full_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
        "    all_logits = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, _ in tqdm(full_loader, desc=f\"Génération des logits pour {model_name}\"):\n",
        "            x = x.to(DEVICE)\n",
        "            all_logits.append(model(x).cpu())\n",
        "    all_logits = torch.cat(all_logits)\n",
        "    torch.save(all_logits, os.path.join(BASE_PATH, f\"{model_name}_logits.pt\"))\n",
        "    print(f\"Logits sauvegardés. Shape: {all_logits.shape}\")"
      ],
      "metadata": {
        "id": "2X0f_yXHX35T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rvOom3sanbq"
      },
      "outputs": [],
      "source": [
        "# ====== 7. RÉSULTATS GLOBAUX ET VISUALISATION ======\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             RÉSULTATS DE L'ENTRAÎNEMENT             \")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Création d'un DataFrame récapitulatif\n",
        "summary_data = []\n",
        "for model_name, f1_scores in performance_history.items():\n",
        "    summary_data.append({\n",
        "        \"model\": model_name,\n",
        "        \"best_f1_weighted\": max(f1_scores) if f1_scores else 0\n",
        "    })\n",
        "df_summary = pd.DataFrame(summary_data).set_index(\"model\")\n",
        "df_summary.to_csv(os.path.join(DRIVE_OUTPUT_PATH, \"recap_scores_modeles_image.csv\"))\n",
        "\n",
        "print(\"Scores finaux par modèle :\")\n",
        "print(df_summary)\n",
        "\n",
        "# Création du graphique comparatif\n",
        "plt.figure(figsize=(12, 7))\n",
        "for model_name, f1_scores in performance_history.items():\n",
        "    plt.plot(range(1, len(f1_scores) + 1), f1_scores, marker='o', linestyle='--', label=model_name)\n",
        "plt.title(\"Comparaison des Scores F1 (Validation) par Époque\")\n",
        "plt.xlabel(\"Époque\")\n",
        "plt.ylabel(\"F1 Score Pondéré (Weighted)\")\n",
        "plt.xticks(range(1, CONFIG[\"max_epochs\"] + 1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTIE 2 : Fusion et Stacking Avancés des Modèles de Computer Vision"
      ],
      "metadata": {
        "id": "mHhCbScEa6yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# NOTEBOOK DE FUSION AVANCÉE ET STACKING POUR MODÈLES DE COMPUTER VISION\n",
        "#\n",
        "# Objectif : Combiner les prédictions des modèles de computer vision entraînés dans le\n",
        "# Notebook 3 pour maximiser les performances.\n",
        "#\n",
        "# Étapes :\n",
        "#   1. Fusion Pondérée : Recherche des poids optimaux avec Optuna pour créer\n",
        "#      un \"ensemble\" de modèles simple mais performant.\n",
        "#   2. Stacking : Utilisation des logits comme features pour entraîner des\n",
        "#      méta-modèles (LGBM, MLP). Le meilleur est sélectionné par validation\n",
        "#      croisée puis entraîné sur toutes les données.\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "Sl90U-EIYAZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna lightgbm -q"
      ],
      "metadata": {
        "id": "pPbAuvQGYB7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montage de Google Drive pour accéder aux fichiers depuis Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7IFj_Pu4YEQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a09166-3178-48fe-a2e7-a4cf137b88c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 1. PRÉPARATION DE L'ENVIRONNEMENT ======\n",
        "\n",
        "# Importation des bibliothèques\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "\n",
        "# PyTorch et Scikit-learn\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "STxBPBoeYFhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 2. MISE EN PLACE DE L'ESPACE DE TRAVAIL ======\n",
        "\n",
        "# Définition des chemins\n",
        "drive_path = \"/content/drive/MyDrive/Colab Notebooks/data_rakuten\"\n",
        "local_path = '/content/data_stacking_vision/'\n",
        "os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "print(\"--- Copie des logits et des fichiers de configuration depuis Google Drive ---\")\n",
        "\n",
        "# Liste des logits générés par le notebook précédent\n",
        "logits_files = [\n",
        "    \"maxvit_base_tf_224_logits.pt\",\n",
        "    \"convnextv2_base_logits.pt\",\n",
        "    \"tf_efficientnetv2_l_logits.pt\",\n",
        "    \"coatnet_rmlp_2_rw_224_logits.pt\",\n",
        "    \"gcvit_base_logits.pt\",\n",
        "    \"true_labels_final.pt\",\n",
        "    \"val_indices.json\",\n",
        "    \"label_mapping_final.json\"\n",
        "]\n",
        "\n",
        "# Copie des fichiers en local pour un accès rapide\n",
        "for filename in logits_files:\n",
        "    src_file = os.path.join(drive_path, filename)\n",
        "    dst_file = os.path.join(local_path, filename)\n",
        "    if os.path.exists(src_file):\n",
        "        shutil.copy(src_file, dst_file)\n",
        "        print(f\"✅ Copié : {filename}\")\n",
        "    else:\n",
        "        print(f\"❌ Manquant : {src_file}\")"
      ],
      "metadata": {
        "id": "NyJL7tGRYHfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 3. CHARGEMENT DES DONNÉES ======\n",
        "\n",
        "# Chargement des logits de chaque modèle dans un dictionnaire\n",
        "all_logits = {}\n",
        "model_names = [f.replace('_logits.pt', '') for f in logits_files if f.endswith('_logits.pt') and 'true_labels' not in f]\n",
        "for name in model_names:\n",
        "    all_logits[name] = torch.load(os.path.join(local_path, f\"{name}_logits.pt\")).cpu().numpy()\n",
        "\n",
        "# Chargement des étiquettes et indices\n",
        "with open(os.path.join(local_path, \"val_indices.json\"), \"r\") as f:\n",
        "    val_idx = np.array(json.load(f))\n",
        "with open(os.path.join(local_path, \"label_mapping_final.json\"), \"r\") as f:\n",
        "    label_mapping = json.load(f)\n",
        "\n",
        "labels_full = torch.load(os.path.join(local_path, \"true_labels_final.pt\")).numpy()\n",
        "class_names = [label_mapping[str(i)][\"label_name\"] for i in range(len(label_mapping))]\n",
        "\n",
        "# Séparation des étiquettes et des logits en ensembles d'entraînement et de validation\n",
        "all_indices = np.arange(len(labels_full))\n",
        "train_idx = np.setdiff1d(all_indices, val_idx)\n",
        "y_train, y_val = labels_full[train_idx], labels_full[val_idx]\n",
        "logits_train = {name: log[train_idx] for name, log in all_logits.items()}\n",
        "logits_val = {name: log[val_idx] for name, log in all_logits.items()}\n",
        "\n",
        "print(f\"\\nDonnées de fusion et de stacking prêtes.\")"
      ],
      "metadata": {
        "id": "BJFrTlE9YOFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 4. FUSION PONDÉRÉE (ENSEMBLING) ======\n",
        "\n",
        "# Nous utilisons Optuna pour trouver la meilleure pondération à appliquer aux\n",
        "# probabilités de chaque modèle afin de maximiser le score F1.\n",
        "\n",
        "def objective_fusion(trial, logits_dict, y_true):\n",
        "    \"\"\"\n",
        "    Fonction objective pour Optuna visant à maximiser le score F1 pondéré.\n",
        "\n",
        "    Cette fonction est appelée par Optuna à chaque essai. Elle suggère un ensemble de\n",
        "    poids pour chaque modèle, normalise ces poids pour qu'ils somment à 1, puis\n",
        "    calcule les prédictions fusionnées en effectuant une moyenne pondérée des\n",
        "    probabilités (obtenues via softmax sur les logits). Le score F1 qui en résulte\n",
        "    est retourné pour être optimisé.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): L'objet d'essai d'Optuna qui gère la suggestion\n",
        "                                    des hyperparamètres (ici, les poids).\n",
        "        logits_dict (dict): Dictionnaire où les clés sont les noms des modèles et\n",
        "                            les valeurs sont les logits (np.ndarray) correspondants.\n",
        "        y_true (np.ndarray): Le tableau des véritables labels pour l'évaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Le score F1 pondéré calculé pour la combinaison de poids actuelle.\n",
        "               Optuna cherchera à maximiser cette valeur.\n",
        "    \"\"\"\n",
        "    weights = [trial.suggest_float(name, 0.0, 1.0) for name in logits_dict.keys()]\n",
        "    s = sum(weights)\n",
        "    if s < 1e-6: return 0.0 # Retourne un mauvais score pour éviter la division par 0\n",
        "\n",
        "    # Normalisation des poids\n",
        "    weights = np.array(weights) / s\n",
        "\n",
        "    # Calcul des probabilités pondérées\n",
        "    fused_probs = np.zeros_like(list(logits_dict.values())[0])\n",
        "    for i, name in enumerate(logits_dict.keys()):\n",
        "        probs = torch.softmax(torch.tensor(logits_dict[name]), dim=1).numpy()\n",
        "        fused_probs += weights[i] * probs\n",
        "\n",
        "    preds = np.argmax(fused_probs, axis=1)\n",
        "    return f1_score(y_true, preds, average=\"weighted\")\n",
        "\n",
        "print(\"\\n--- Recherche des poids de fusion optimaux avec Optuna ---\")\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(lambda trial: objective_fusion(trial, logits_val, y_val), n_trials=150)\n",
        "\n",
        "# Évaluation avec les meilleurs poids trouvés\n",
        "best_weights = np.array([study.best_params[name] for name in model_names])\n",
        "best_weights /= sum(best_weights)\n",
        "fused_probs_val = sum(best_weights[i] * torch.softmax(torch.tensor(logits_val[name]), dim=1).numpy() for i, name in enumerate(model_names))\n",
        "preds_fusion = np.argmax(fused_probs_val, axis=1)\n",
        "f1_fusion = f1_score(y_val, preds_fusion, average='weighted')\n",
        "\n",
        "print(f\"\\nMeilleur F1-Score (Fusion Pondérée) : {f1_fusion:.5f}\")\n",
        "print(\"Poids optimaux :\")\n",
        "for name, weight in zip(model_names, best_weights):\n",
        "    print(f\"  - {name}: {weight:.4f}\")"
      ],
      "metadata": {
        "id": "ZjXlwh2oYQiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 5. STACKING AVEC UN MÉTA-MODÈLE ======\n",
        "\n",
        "# Le stacking utilise les logits des modèles de base comme \"features\" pour\n",
        "# entraîner un méta-modèle. Cela permet de capturer des relations plus complexes\n",
        "# entre les prédictions des modèles de base.\n",
        "\n",
        "# --- 5.1. Préparation des données pour le Stacking ---\n",
        "X_train_stack = np.concatenate(list(logits_train.values()), axis=1)\n",
        "X_val_stack = np.concatenate(list(logits_val.values()), axis=1)\n",
        "\n",
        "print(f\"\\nShape des features de Stacking (Train): {X_train_stack.shape}\")\n",
        "print(f\"Shape des features de Stacking (Val):   {X_val_stack.shape}\")\n",
        "\n",
        "\n",
        "# --- 5.2. Sélection du meilleur méta-modèle par validation croisée ---\n",
        "# Nous évaluons LightGBM et un MLP pour voir lequel est le plus performant\n",
        "# sur nos données de stacking.\n",
        "\n",
        "def train_lgbm(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Entraîne et évalue un classifieur LightGBM.\n",
        "\n",
        "    Cette fonction utilitaire initialise un modèle LGBMClassifier, l'entraîne sur\n",
        "    les données d'entraînement fournies et retourne le score F1 pondéré calculé\n",
        "    sur l'ensemble de validation.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Features d'entraînement (logits concaténés).\n",
        "        y_train (np.ndarray): Labels d'entraînement.\n",
        "        X_val (np.ndarray): Features de validation.\n",
        "        y_val (np.ndarray): Labels de validation.\n",
        "\n",
        "    Returns:\n",
        "        float: Le score F1 pondéré du modèle sur les données de validation.\n",
        "    \"\"\"\n",
        "    model = lgb.LGBMClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
        "    preds = model.predict(X_val)\n",
        "    return f1_score(y_val, preds, average=\"weighted\")\n",
        "\n",
        "print(\"\\n--- Évaluation des méta-modèles par validation croisée (5-fold) ---\")\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "lgbm_scores = []\n",
        "\n",
        "for fold, (train_fold_idx, val_fold_idx) in enumerate(skf.split(X_train_stack, y_train)):\n",
        "    X_train_fold, X_val_fold = X_train_stack[train_fold_idx], X_train_stack[val_fold_idx]\n",
        "    y_train_fold, y_val_fold = y_train[train_fold_idx], y_train[val_fold_idx]\n",
        "\n",
        "    score = train_lgbm(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "    lgbm_scores.append(score)\n",
        "    print(f\"Fold {fold+1}/5 - F1 Score LGBM : {score:.4f}\")\n",
        "\n",
        "avg_lgbm_score = np.mean(lgbm_scores)\n",
        "print(f\"\\nScore F1 moyen pour LightGBM (CV) : {avg_lgbm_score:.5f}\")\n",
        "# (Note: une CV similaire pourrait être faite pour un MLP, mais LGBM est souvent\n",
        "# un excellent point de départ pour les données tabulaires comme les logits).\n",
        "\n",
        "\n",
        "# --- 5.3. Entraînement et Évaluation du Méta-Modèle Final ---\n",
        "print(\"\\n--- Entraînement du méta-modèle final (LGBM) sur toutes les données d'entraînement ---\")\n",
        "\n",
        "final_lgbm_model = lgb.LGBMClassifier(\n",
        "    objective=\"multiclass\",\n",
        "    num_class=len(class_names),\n",
        "    metric=\"multi_logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "final_lgbm_model.fit(X_train_stack, y_train, eval_set=[(X_val_stack, y_val)])\n",
        "preds_lgbm_final = final_lgbm_model.predict(X_val_stack)"
      ],
      "metadata": {
        "id": "8JcG73CaYTj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 6. RÉSULTATS FINAUX ET SAUVEGARDE ======\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"              RÉSULTATS FINAUX              \")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Score F1 (Fusion Pondérée Simple) : {f1_fusion:.5f}\")\n",
        "\n",
        "print(\"\\n--- Rapport de Classification Final (Stacking avec LightGBM) ---\")\n",
        "print(classification_report(y_val, preds_lgbm_final, target_names=class_names, digits=4))\n",
        "\n",
        "# Sauvegarde du méta-modèle final, qui est l'artefact le plus précieux.\n",
        "meta_model_path = os.path.join(drive_path, 'meta_model_vision_lgbm.pkl')\n",
        "joblib.dump(final_lgbm_model, meta_model_path)\n",
        "print(f\"\\n✅ Méta-modèle final sauvegardé sur Drive : {meta_model_path}\")"
      ],
      "metadata": {
        "id": "OYxF9IXJa7Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > \"/content/drive/MyDrive/Colab Notebooks/requirements_Images.txt\""
      ],
      "metadata": {
        "id": "iwh7XR61USeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}